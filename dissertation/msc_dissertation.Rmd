---
title: |
  | __How analysis strategy affects analysis results__
subtitle: |
  | *Assessing the median effect robustness in @silberzahn2018 through model specification*
  |
  | Master's thesis - DRAFT
author: "Sebastian Ploner, sploner1@sheffield.ac.uk, University of Sheffield"
linestretch: 1.5
output:
    pdf_document:
        includes:
            in_header: header.tex
        extra_dependencies: ["flafter"]
bibliography: references.bib
csl: apa.csl
header-includes:
  - \usepackage[ruled,vlined]{algorithm2e}
---

```{r setup, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)

#...............................................# Set parameters #..............................................#

# This script can be run in its entirety to reproduce the specification curve analysis for one population at a time. 
# Please read the comments in this section to understand what each parameter does.
# After setting the parameters for the first time, we recommend reading the WARNINGS on lines 1 and 56 before
# sourcing the entire script or running the "Prepare R Session" section.

#... Set seed for random number generation
set.seed(999999) # set to 999999 to reproduce the results reported in the paper


#...........................................# Prepare R Session #...............................................#

# WARNING: All currently loaded packages will be detached and packages needed 
#          to run this script will be loaded. 
#          Missing packages will be downloaded and installed.
#          Also, the working directory will be automatically set to the location of this script.
#          If you did not alter the structure of the folder containing this script and the data,
#          in most cases you should be able to run this script without any manual adjustments 
#          in a current version of RStudio (tested in RStudio Version 1.1.453 and newer releases running 
#          R Version 3.5.1 and newer releases).


# Detach packages
if(!is.null(sessionInfo()$otherPkgs)) {
  invisible(lapply(paste("package:", names(sessionInfo()$otherPkgs), sep = ""), 
                   detach, character.only = TRUE, unload = TRUE))
}


# Download and install missing packages
requiredPackages = c("here", "data.table", "tidyverse", "PupillometryR", "cowplot", "knitr", "kableExtra", "papeR", "DiagrammeR")

missingPackages = requiredPackages[!requiredPackages %in% installed.packages()[ , "Package"]]

if(length(missingPackages)) {
  install.packages(missingPackages)
}


# Load required packages
invisible(lapply(requiredPackages, require, character.only = TRUE))


# Load data
dat = data.frame(fread(here::here("data", "3_prepared_data.csv")))

```

```{=latex}

% Trigger ToC creation in LaTeX
\renewcommand{\baselinestretch}{1.5}\normalsize
\tableofcontents
\renewcommand{\baselinestretch}{1.5}\normalsize

```

... \newline
*Abstract--in progress.* \newline
... \newline


\newpage


# Introduction
     The Covid-19 pandemic has reaffirmed the need for sound scientific research to inform decision-making on a scientific and societal level [@collins2021]. Rigorous research builds upon a systematic and well-reasoned approach to solving a research problem. Based on available literature, a falsifiable research question is defined and a corresponding hypothesis is developed. An appropriate study is then designed and conducted. To draw inferences from the gathered data, statistical models are developed and the effect of each variable is assessed. Every one of these steps is influenced by the decisions researchers make, which are known as *researcher degrees of freedom* [RDF, @simmons2011; @wicherts2016]. Usually there are many feasible analytical strategies to answering a research question and none of them are inherently right or wrong  [@carp2012]. This often creates uncertainty, for example, about what covariates to include and how to model them, which in turn leads to inconsistent findings [@patel2015; @ioannidis2008]. Recently, efforts have been made to better understand the scope of variation induced by different analytical strategies.

     These efforts include a crowdsourcing approach to data analysis [@silberzahn2018; @botvinik-nezer2020]. Its premise is to have a large number of researchers team up into to smaller, independent groups to investigate the same research question based on the same dataset. For instance, @silberzahn2018 had 29 teams investigate the effects of a football player's skin colour on the odds of being sent off the field. The variation between the analytical strategies was substantial. There were 29 different analyses with 21 different combinations of covariates. Twenty teams found a statistically significant effect. The authors also controlled for researchers' prior believes and experience as well as peer-rated analysis quality, none of which accounted for the variation of results. @botvinik-nezer2020 made similar observations. Crowdsourcing data analysis excels at emphasising the substantial impact of different analytical strategies, but has a significant drawback. It is extremely time and personnel intensive, the two mentioned studies lasted between 2 to 3+ years, and included 61 and 180 analysts, respectively. Not to mention the organisational effort.

     @silberzahn2018 and @botvinik-nezer2020, therefore, suggest using multiverse analysis (also known as specification-curve analysis) as an alternative to crowdsourcing. This approach requires identifying and running all plausible analytical strategies. Plausible strategies are defined as statistically valid, non-redundant, tests appropriate to the research question. Their aggregated results are then used to make inferences about the research question. Despite being statistically more complex and computationally more intense, this approach has the advantages of only needing one, or ideally a couple, researchers. Moreover, a researcher's inherent strategy bias is neutralised and noise is made transparent [@simonsohn2020]. Multiverse approaches are therefore well suited for assessing the scope of variation induced by different analytical strategies. For instance, @patel2015 assessed an effect's robustness by running every combination of covariates and modelling options. Their conclusion was the larger the variation of outcomes, the less robust is the effect, and the less it should therefore to be trusted. The authors termed this measure the *vibration of effect* (VoE).

     Taken together, researchers make many analytical decisions throughout conduction a study which are know as *researcher degrees of freedom*. Crowdsourcing analysis presents a strategy to understand their induced variation, but is impractical due to its substantial time and personnel requirements. Multiverse analysis present a promising, low-personnel, but computationally intense alternative to assessing the scope this variation. Despite observing substantial variation in crowdsourced data analysis projects little is known about the extend to which these approaches cover the full range of possible results. In this project, using methods of multiverse analysis I will define the results space of @silberzahn2018 to estimate study's representativeness. Based on these insights I aim at contributing to further understanding the value, limitations and mechanisms of the multiverse approach. 


# Analysis
## Analysis plan
     The project's objective is to define the results space of the @silberzahn2018 to the estimate study's representativeness. The results space refers to the numerical interval between the lowest and highest possible outcome. It is created by running every possible analytical strategy. Hence, it is closely related to assessing an effect's robustness. @patel2015 developed a standardised approach to assessing an effect's robustness which they termed assessing the *vibration of effect*. I will therefore use this standardised approach to define the results space. The approach essentially comprises two parameters: the statistical models and the covariates (or control variables). In @silberzahn2018 the analysts used numerous different statistical models like multiple linear regression, mixed-model logistic regression or Bayesian logistic regression. In total there were 29 different modelling approaches. Additionally, each team used a different set of covariates. Across all teams there 15 covariates used. This gives $2^n$ i.e., $2^{15} = 32,768$ possible combinations of covariates. Adding all modelling possibilities to the equation gives a total of $29 * 2^{15} = 950,272$ combinations to run. This number exceeds the project's scope, hence, it needs to be reduce to a more manageable count. I am, therefore, focusing on one modelling approach. In particular, on the approach that produced the median outcome of all analyses. The median, being the middle number of any set of values, is a reasonable starting point in order to estimate the results space. Nevertheless, even focusing on one analytical approach still leaves $1 * 2^{15} = 32,768$ possibilities. Due to computational limitations I will first run a sample of 200, followed by a sample of 1,000 combinations. If there's a substantial difference between the median and the spread I will run more models, if not, I will make the assumption that 1,000 reasonably estimates all ~33k combinations.

     In @silberzahn2018 the median outcome was produced by @team23 which will hereinafter be referred to as *team 23* due to it's designated team number in the original study. *Team 23* first transformed the data and then conducted a mixed-model logistic regression. The first step of this project will be replicating the transformation and the analysis. Replicating other researchers analyses has the benefit of checking their work and, if results are indeed replicated, increasing confidence in them. It also ensures that this project has the same starting point as *team 23* did. Given the team made all their scripts publicly available I expect this step to be straightforward. Next is drawing a random sample of covariates, without replacement. "Without replacement" ensures all covariate combinations are unique in the sample. The covariates will be appended to the base (or core) variables. Base variables are those variables that are primarily assessed to answer the research question. In this case whether a football player's skin colour effects the odds of being sent off the field. *Team 23* defined two interaction terms as the base: "skin tone *X* implicit bias" and "skin tone *X* explicit bias". (The variables are described in the "data" section.) Hence, all models will have the following structure:
$$
\begin{aligned}
RedCard = SkinTone*ImplicitBias + SkinTone*ExplicitBias + CovariateCombination_{i}
\end{aligned}
$$ 
While running each model, the relevant statistics will be extracted. Those are the coefficient (or effect) of skin tone, its standard error, test-statistic and p-value. Just as *team 23* did I will then calculate the 95% confidence intervals (CI). The estimates and their CIs are then transformed to odds ratios (OR) through exponentiating them to the power of two. Odds ratios quantify the strength of association between two variables. If greater than one the dependent variable is more likely to occur given the independent variable, if lower than one it less likely. The odds ratios will be visualised to assessed through a conventional specification curve plot, a rain-cloud plot and scatter plot. The following describes them in more detail:

(i) The specification curve plot was developed to be a descriptive plot i.e., raw data is being plotted without any aggregation done. Its objective is describe the results space while allowing the reader to identify the specified covariate for each outcome [@simonsohn2020]. It therefore has two vertically stacked components. The top part shows a sorted scatter plot. On the horizontal axis are the models specifications and on the vertical axis is the outcome measure. The points are sorted from lowest to highest outcome measure. This way the lowest outcome measure is on the bottom left corner and the highest in the top right one. The bottom part of the plot represents are table. Just like the top the horizontal axis hold the model specifications, the vertical axis lists the covariates. This arrangement allows each cell to specify the presence or absence for each covariate in a given specified model. For the top and bottom plot to work together it is imperative that the horizontal axes are identically arranged. If so, for every point in the top plot (i.e., for every outcome) the specified covariates can be seen in the bottom plot. For an example check the results section. The issue, however, is that if too many specifications are on the horizontal axis it becomes very hard to identify specific covariates. Hence, I also included the rain-cloud and scatter plot.

(ii) The rain-cloud plot was developed by @allen2021. Its objective is to give an unbiased, transparent view on the raw data. It therefore combines a density, box and scatter plot. If stacked vertically from top to bottom, respectively, they look like a cloud with rain drops, hence, its name. The advantage of this plot is to be able to assess the raw data including key statistics (median, 25th and 75th quartiles and CIs) as well as the probability density all in one succinct plot. Given that this project seeks to define and describe a results space the rain-cloud plot the ideal graph to do this. However, the plot neither allows to identify the covariates giving rise to the outcome nor their specified effects. (For an example check the results section.) Therefore a third plot is developed which is to be assessed in tandem with this plot.

(iii) The third and final plot is a sorted scatter plot. While there is nothing special about the plot itself its content is important. Given the very high number of models that are run, it does not make sense to assess each model and its effects in isolation. Instead the effects for each covariates neeed to be aggregated. Hence, the goal of this plot is to viusalise the specified effect for each covariate. Prior to doing so, the coviates first need to be aggregated and their specified effects need to be calucualted. To this end, each covariate is recoded into a binary factor: included in the model yes/no. This newly defined factors are than used as an independent variables in an ANOVA. The previously calcualted odds rations are used as dependent variables. The estimates of the fitted model are the specified effect for each covariates. These are visualised similarly to the top part of the specification curve plot. The covariates are on the horizental axis, will the odds ratios are on the vertical axis. Here too, the lowest oucome is in the bottom left corner and the highest outcome in the top right one. Additionally the point are coloured indicating a statistically significant effect (or non-signifcant).

## Code
     The code is based on @haessler2020 and @patel2015. The former provided the operational backbone of the code. The latter the idea for the algorithm. R's random number generator is first fixed to be able to reproduce the outcomes. Then, all the packages are detached and only the relevant ones are loaded to prevent other functions from interfering. If the required packages are not installed, the code will do so. Next, the prepared data is loaded using the "Here" package [@here] for operating systems independence and the "data.table" package for faster importing of the data [@datatable]. (The data has been prepared by running the "disaggregate_v3.py" script of *team 23*; for more detail see the "data" section.) After having loaded the data the variables are defined as dependent, base (or core) variables and covariates. This includes defining the variables' types: numeric, factor or random effect. Next, a matrix is created established all possible combination of covariates (including base variables). Each row represents one combination, the columns represent the variables. The cells are set to TRUE or FALSE to indicate a variables presence or absence, respectively. Thus, the matrix structure looks as the following:

$$\begin{bmatrix}
\text{dv} & \text{base var}_{1} & \text{base var}_{2} & \text{base var}_{3} & \text{covar}_{1} & \text{covar}_{2} & \text{covar}_{...} & \text{covar}_{15} \\
\text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{FALSE} & \text{FALSE} & \text{...} & \text{FALSE} \\
\text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{FALSE} & \text{...} & \text{FALSE} \\
\text{...} & \text{...} & \text{...} & \text{...} & \text{...} & \text{...} & \text{...} & \text{...} \\
\text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{TRUE} & \text{...} & \text{TRUE} \\
\end{bmatrix}$$ \newline


     Using the *apply* function, looping over each row, the variables are pasted together thereby creating the corresponding formulas which are then saved as a vector and are appended as column to the aforementioned matrix.

     *Team 23* ran a mixed-model logistic regression analysis using the "lme4" package more specifically, its "glmer" function [@lme4]. The function only works if a random structure is included in the formula. As all possible covariate combinations are explored, a random structure is not included in every formula. Therefore, before running the models, the formulas need to separated based on including a random structure or not. The random structures were defined at the beginning of the code using the following syntax: "(1|*covariate*)". Hence, this pattern is also used to identify random structures through the "grepl" function. For the non-random structure formulas the regular "glm" function is run. Both models, mixed-effect and "regular", are estimated using Maximum Likelihood estimation to ensure the outcomes are as comparable as possible. *Team 23* had also set the "nAGQ" parameter to zero (for the mixed-model) this sacrificed parameter estimation accuracy for speed. The same was done here to stay as close to the original analysis as possible. Finally, 1,000 formula samples were drawn. 75% of all formulas include at least one random structure this proportion is reflected in the sample. Furthermore, the samples are drawn without replacement i.e., every formula is unique.

     Running the models yielded another challenge. Not only were the running times per model at times above 30 minutes, storing the model summaries also required significant memory. For instance, testing the code with a minimal sample of 10 (random structure) formulas yielded an analysis output larger than 400MB. This does not scale well, hence, a wrapper function was build. It's purpose was to extract the estimates, standard errors, test-statistics and p-values for the variable of interest (in this case: skin tone). This effect was substantial. The final outcome object for 1,000 models was reduced to 0.3MB(!).

     The models were run using the "lapply" function more specifically, the paralellised version thereof including progress bars ("pbmcapply", @pbmcapply). Parallisation was necessary in order to reduce running time. The standard R is set to use one core, only. Parallelising the processes to six cores decreased the running time by six-fold. The results of each model iteration was appended to a list. Hence, the outcomes were two lists, one from the mixed-models, and one from the "regular" models. These lists were transformed to data frames and merged. Moreover, the p-values was coded into significant and non-significant ($\alpha = 0.05$). The odds ratios as well as their confidence intervals were calculated (the same way *team 23* did it). The resulting data frame was save as a comma-separated values (CSV) file. 

\begin{algorithm}
\caption{Pseudocode overview--in progress}
\DontPrintSemicolon
\SetAlgoLined
\KwData{Prepared data based on team 23 script}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output} 
$variables$ $\leftarrow$ Define dependent, base variables and covariates \\
$specifications$ $\leftarrow$ Use $variables$ to create matrix containing all possible covariate combinations \\
$formula$ $\leftarrow$ Paste $specifications$ by row and append as column to $specifications$ \\
$formula.ranef; formula.ef$ $\leftarrow$ Separate formulas based on including a random-structure \\
$samlple.ranef; sample.ef$ $\leftarrow$ Random, without replacement, sample from $formula.ranef; formula.ef$
\BlankLine
 initialization\;
  \While{not at end of this document}{
    read current\;
    \eIf{understand}{
      go to next section\;
      current section becomes this one\;
      }{
      go back to the beginning of current section\;
      }
    }
\KwResult{List containing model statistics}
\end{algorithm} 


... \newline
*Explaining why not adjusting for multiple comparison--in progress* \newline
... \newline


To better understand the specified effects of the covariates I ran an ANOVA where each covariate was coded into included yes/no. The above retrieved effects where used outcome measure. The results were stored and saved as a separate data frame. As a final step the graphs were created using the "tidyverse" (specifically, "ggplot", @tidyverse), "PupillometryR" [@PupillometryR] and "cowplot" [@cowplot] packages. \newline


... \newline
*Explaining how the graphs were created including necessary data transformation needed--in progress.* \newline
... \newline


The code is written in R (Version 1.4.1103) on macOS Big Sur (Version 11.4) and can be retrieved from my [*GitHub repository*](https://github.com/sebastianplnr/msc_dissertation_project). 


## Data origin and preparation
Outline:

* The same dataset as in @silberzahn2018 is used.

* What is the data about

* Data origin i.e., how did silberzahn et al. (2018) come up with the data set

* how was the data processed by team 23

* summary statistics of the data set

```{r, echo = FALSE}

# Setting variable class
dat$player = as.factor(dat$player)
dat$specific_pos = as.factor(dat$specific_pos)
dat$club = as.factor(dat$club)
dat$league_country = as.factor(dat$league_country)
dat$ref = as.factor(dat$ref)
dat$ref_country = as.factor(dat$ref_country)

# Building a summary table
kable(papeR::summarise(dat,
                       type = "numeric",
                       variables = c("height_cm", "weight_kg", "age_yrs", "games", "goals", "victories", "ties", "all_reds", "player_cards_received",
                                     "ref_cards_assigned", "skin_tone_num", "imp_bias", "exp_bias")),
      format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = "scale_down")

```


\newpage
## Outcome I: vibration of effect
## Outcome II: specified covariate effects
## Outcome III: specification curve


\newpage
# Discussion


\newpage
# References